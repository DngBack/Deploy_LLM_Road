{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXkR4NjYhezg"
      },
      "source": [
        "# Run Qwen PPO with [verl](https://github.com/volcengine/verl)\n",
        "\n",
        "This tutorial provides a step-by-step guide to using verl for executing your RLHF pipeline. You can find our [github repo](https://github.com/volcengine/verl/) and [documentation](https://verl.readthedocs.io/en/latest/index.html) for mode details.\n",
        "\n",
        "This notebook is also published on the [Lightning Studio](https://lightning.ai/hlin-verl/studios/verl-getting-started) platform, which provides free GPU quota every month. Checkout the published notebook with pre-installed dependencies using a free L4 GPU [here](https://lightning.ai/hlin-verl/studios/verl-getting-started) (no credit card required).\n",
        "\n",
        "### You will learn:\n",
        "\n",
        "- How to install verl from scratch.\n",
        "- How to use existing scripts to run an RLHF pipeline with your own models and data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSDNzNuQkJJh"
      },
      "source": [
        "# Dependency Installation\n",
        "\n",
        "If you are running on Lightning Studio using the published notebook, the dependencies are **already installed** and you can proceed to step \"**Load Pretrained Language Model**\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnfZyMm-3BNC",
        "outputId": "059a6bd7-0cf2-4176-e92c-f2557957af0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.8.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Collecting torch==2.4.0\n",
            "  Using cached torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.19.0\n",
            "  Using cached torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Using cached triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (11.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m144.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m155.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchvision-0.19.0 triton-3.0.0\n",
            "torch                              2.4.0\n",
            "torchaudio                         2.5.1+cu124\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.19.0\n",
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.4.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187805408 sha256=92cf49e6f66795b6934cec0cba526ed6e45d3313de3f905d45df8773f19092a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.7.4.post1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade pip setuptools wheel\n",
        "!pip3 install torch==2.4.0 torchvision==0.19.0\n",
        "!pip3 list | grep torch\n",
        "!pip3 install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzV28CwOmruV"
      },
      "source": [
        "## Install and verify verl\n",
        "Now we're ready to install verl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mtIn1VOk2E7",
        "outputId": "e7332d73-affa-4bbf-aefd-dc3ae256e467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/root/verl_repo' already exists and is not an empty directory.\n",
            "Obtaining file:///root/verl_repo\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from verl==0.2) (1.3.0)\n",
            "Collecting codetiming (from verl==0.2)\n",
            "  Downloading codetiming-1.4.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting datasets (from verl==0.2)\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dill (from verl==0.2)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting hydra-core (from verl==0.2)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from verl==0.2) (1.26.4)\n",
            "Collecting pybind11 (from verl==0.2)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting ray>=2.38 (from verl==0.2)\n",
            "  Downloading ray-2.42.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting tensordict (from verl==0.2)\n",
            "  Downloading tensordict-0.7.0-cp311-cp311-manylinux1_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting transformers<4.48 (from verl==0.2)\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting vllm<=0.6.3 (from verl==0.2)\n",
            "  Downloading vllm-0.6.3-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from verl==0.2) (0.14.0)\n",
            "Collecting liger-kernel (from verl==0.2)\n",
            "  Downloading liger_kernel-0.5.3-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pylatexenc (from verl==0.2)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (4.25.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray>=2.38->verl==0.2) (2.32.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.48->verl==0.2) (0.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.48->verl==0.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.48->verl==0.2) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.48->verl==0.2) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers<4.48->verl==0.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (0.2.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (3.11.12)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (1.61.1)\n",
            "Collecting uvicorn[standard] (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (2.10.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (11.1.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (0.21.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lm-format-enforcer==0.10.6 (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting outlines<0.1,>=0.0.43 (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (4.12.2)\n",
            "Collecting partial-json-parser (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (24.0.1)\n",
            "Collecting msgspec (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf==0.10.0 (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (8.6.1)\n",
            "Collecting mistral-common>=1.4.4 (from mistral-common[opencv]>=1.4.4->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading mistral_common-1.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (0.8.1)\n",
            "Collecting nvidia-ml-py (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (2.4.0)\n",
            "Requirement already satisfied: torchvision==0.19 in /usr/local/lib/python3.11/dist-packages (from vllm<=0.6.3->verl==0.2) (0.19.0)\n",
            "Collecting xformers==0.0.27.post2 (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from vllm<=0.6.3->verl==0.2)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.10.6->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm<=0.6.3->verl==0.2) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm<=0.6.3->verl==0.2) (12.5.82)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->verl==0.2) (17.0.0)\n",
            "Collecting dill (from verl==0.2)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->verl==0.2) (2.2.2)\n",
            "Collecting xxhash (from datasets->verl==0.2)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->verl==0.2)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core->verl==0.2)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core->verl==0.2)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of tensordict to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensordict (from verl==0.2)\n",
            "  Downloading tensordict-0.6.2-cp311-cp311-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
            "  Downloading tensordict-0.6.1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
            "  Downloading tensordict-0.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
            "  Downloading tensordict-0.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from tensordict->verl==0.2) (3.1.1)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from tensordict->verl==0.2) (3.10.15)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm<=0.6.3->verl==0.2) (2.4.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm<=0.6.3->verl==0.2) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm<=0.6.3->verl==0.2) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm<=0.6.3->verl==0.2) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm<=0.6.3->verl==0.2) (1.18.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray>=2.38->verl==0.2) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray>=2.38->verl==0.2) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray>=2.38->verl==0.2) (0.22.3)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common[opencv]>=1.4.4->vllm<=0.6.3->verl==0.2) (4.11.0.86)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm<=0.6.3->verl==0.2) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm<=0.6.3->verl==0.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm<=0.6.3->verl==0.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm<=0.6.3->verl==0.2) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm<=0.6.3->verl==0.2) (1.3.1)\n",
            "Collecting lark (from outlines<0.1,>=0.0.43->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm<=0.6.3->verl==0.2) (1.6.0)\n",
            "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm<=0.6.3->verl==0.2) (0.61.0)\n",
            "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm<=0.6.3->verl==0.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm<=0.6.3->verl==0.2) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray>=2.38->verl==0.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray>=2.38->verl==0.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray>=2.38->verl==0.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray>=2.38->verl==0.2) (2025.1.31)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->vllm<=0.6.3->verl==0.2) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->verl==0.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->verl==0.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->verl==0.2) (2025.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm<=0.6.3->verl==0.2) (0.14.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm<=0.6.3->verl==0.2)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm<=0.6.3->verl==0.2) (14.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm<=0.6.3->verl==0.2) (1.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->verl==0.2) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0->vllm<=0.6.3->verl==0.2) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm<=0.6.3->verl==0.2) (0.44.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0->vllm<=0.6.3->verl==0.2) (1.3.0)\n",
            "Downloading ray-2.42.1-cp311-cp311-manylinux2014_x86_64.whl (67.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m163.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vllm-0.6.3-cp38-abi3-manylinux1_x86_64.whl (193.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.5/193.5 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "Downloading lm_format_enforcer-0.10.6-py3-none-any.whl (43 kB)\n",
            "Downloading xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m139.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading codetiming-1.4.0-py3-none-any.whl (7.2 kB)\n",
            "Downloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Downloading liger_kernel-0.5.3-py3-none-any.whl (113 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Downloading tensordict-0.5.0-cp311-cp311-manylinux1_x86_64.whl (337 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "Downloading mistral_common-1.5.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m148.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "Downloading outlines-0.0.46-py3-none-any.whl (101 kB)\n",
            "Downloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl (18 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
            "Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "Downloading pyairports-2.1.1-py3-none-any.whl (371 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "Building wheels for collected packages: verl, antlr4-python3-runtime, pylatexenc\n",
            "  Building editable for verl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for verl: filename=verl-0.2-0.editable-py3-none-any.whl size=13318 sha256=defffaca2cb3e3f1f85433aca0f62ba6cd619d5c8084a89c2942510061a0f373\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wll96nt8/wheels/7d/8c/cc/47bd8067523d446b7c410662c936c6abe4ab7aa3131bc48930\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144591 sha256=6291d997fdea436395567942a8f9d5b7e81c0d3373c33ea79b760c4ffe7606d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136863 sha256=82027adc3dcad63871d46c0b90e6657c1f92498479a3582207730a4d035ac888\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/7a/33/9fdd892f784ed4afda62b685ae3703adf4c91aa0f524c28f03\n",
            "Successfully built verl antlr4-python3-runtime pylatexenc\n",
            "Installing collected packages: pylatexenc, pyairports, nvidia-ml-py, antlr4-python3-runtime, xxhash, uvloop, uvicorn, python-dotenv, pycountry, pybind11, partial-json-parser, omegaconf, msgspec, lark, interegular, httptools, gguf, diskcache, dill, codetiming, watchfiles, tiktoken, starlette, multiprocess, hydra-core, prometheus-fastapi-instrumentator, lm-format-enforcer, fastapi, xformers, transformers, tensordict, ray, mistral-common, liger-kernel, datasets, outlines, vllm, verl\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 codetiming-1.4.0 datasets-3.3.0 dill-0.3.8 diskcache-5.6.3 fastapi-0.115.8 gguf-0.10.0 httptools-0.6.4 hydra-core-1.3.2 interegular-0.3.3 lark-1.2.2 liger-kernel-0.5.3 lm-format-enforcer-0.10.6 mistral-common-1.5.3 msgspec-0.19.0 multiprocess-0.70.16 nvidia-ml-py-12.570.86 omegaconf-2.3.0 outlines-0.0.46 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.0.2 pyairports-2.1.1 pybind11-2.13.6 pycountry-24.6.1 pylatexenc-2.10 python-dotenv-1.0.1 ray-2.42.1 starlette-0.45.3 tensordict-0.5.0 tiktoken-0.9.0 transformers-4.47.1 uvicorn-0.34.0 uvloop-0.21.0 verl-0.2 vllm-0.6.3 watchfiles-1.0.4 xformers-0.0.27.post2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# In case you run this notebook and have not cloned verl yet:\n",
        "!git clone https://github.com/volcengine/verl $HOME/verl_repo\n",
        "\n",
        "!cd $HOME/verl_repo && pip3 install -e . -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dARh6c4_ZEo0"
      },
      "source": [
        "## Restart the python kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnissvsWZEo1",
        "outputId": "baba9320-0e16-4798-f1de-46eb1447d4e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "# Restart the kernel to pickup the latest python packages\n",
        "IPython.get_ipython().kernel.do_shutdown(restart=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOBX8Jqc-ZBe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "  torch.ones(1, dtype=torch.bfloat16).cuda()\n",
        "except AssertionError:\n",
        "  print(\"Please switch to an env with GPUs supporting bfloat16 (L4 RTX 5000, A5000, A100, H100, A10, etc)\")\n",
        "\n",
        "try:\n",
        "  import verl\n",
        "except Exception as e:\n",
        "  print(\"Please install verl via pip and restart the kernel\")\n",
        "  raise e\n",
        "\n",
        "import flash_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mawNxDfo3Uu"
      },
      "source": [
        "# Load Pretrained Language Model\n",
        "\n",
        "verl supports models available in Huggingface transformers (as well as custom Megatron models).\n",
        "\n",
        "Let's download the model first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8FsgBYnpR-R",
        "outputId": "b008336f-9a1b-4b9d-d80d-ac13a9ffc8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFetching 10 files:   0% 0/10 [00:00<?, ?it/s]Downloading 'tokenizer.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.443909a61d429dff23010e5bddd28ff530edda00.incomplete'\n",
            "Downloading 'LICENSE' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/DhCjcNQuMpl4FL346qr3tvNUCgY=.6634c8cc3133b3848ec74b9f275acaaa1ea618ab.incomplete'\n",
            "Downloading 'generation_config.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.dfc11073787daf1b0f9c0f1499487ab5f4c93738.incomplete'\n",
            "Downloading 'merges.txt' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/PtHk0z_I45atnj23IIRhTExwT3w=.20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0.incomplete'\n",
            "Downloading 'config.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.0dbb161213629a23f0fc00ef286e6b1e366d180f.incomplete'\n",
            "Downloading 'README.md' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.4b8373851d093eb9f3017443f27781c6971eff24.incomplete'\n",
            "Downloading 'model.safetensors' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.fdf756fa7fcbe7404d5c60e26bff1a0c8b8aa1f72ced49e7dd0210fe288fb7fe.incomplete'\n",
            "\n",
            "model.safetensors:   0% 0.00/988M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.46MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/generation_config.json\n",
            "\n",
            "\n",
            "config.json: 100% 659/659 [00:00<00:00, 5.35MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/config.json\n",
            "\n",
            "\n",
            "merges.txt:   0% 0.00/1.67M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json:   0% 0.00/7.03M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "LICENSE: 100% 11.3k/11.3k [00:00<00:00, 37.6MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/LICENSE\n",
            "Downloading 'tokenizer_config.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.07bfe0640cb5a0037f9322287fbfc682806cf672.incomplete'\n",
            "Downloading 'vocab.json' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/j3m-Hy6QvBddw8RXA1uSWl1AJ0c=.4783fe10ac3adce15ac8f358ef5462739852c569.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 26.3MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/tokenizer_config.json\n",
            "\n",
            "model.safetensors:   1% 10.5M/988M [00:00<00:25, 38.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vocab.json:   0% 0.00/2.78M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 5.97MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/merges.txt\n",
            "\n",
            "model.safetensors:   2% 21.0M/988M [00:00<00:23, 41.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 14.8MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/tokenizer.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 7.63MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/vocab.json\n",
            "Downloading '.gitattributes' to '/root/models/Qwen2.5-0.5B-Instruct/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "\n",
            "model.safetensors:   3% 31.5M/988M [00:00<00:22, 41.9MB/s]\u001b[A\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 8.03MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/.gitattributes\n",
            "Fetching 10 files:  10% 1/10 [00:01<00:09,  1.03s/it]\n",
            "model.safetensors:   4% 41.9M/988M [00:00<00:21, 43.1MB/s]\u001b[A\n",
            "\n",
            "README.md: 100% 4.92k/4.92k [00:00<00:00, 22.7MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/README.md\n",
            "Fetching 10 files:  30% 3/10 [00:01<00:02,  2.77it/s]\n",
            "model.safetensors:   5% 52.4M/988M [00:01<00:21, 43.0MB/s]\u001b[A\n",
            "model.safetensors:   6% 62.9M/988M [00:01<00:21, 43.0MB/s]\u001b[A\n",
            "model.safetensors:   7% 73.4M/988M [00:01<00:21, 42.9MB/s]\u001b[A\n",
            "model.safetensors:   8% 83.9M/988M [00:01<00:20, 43.1MB/s]\u001b[A\n",
            "model.safetensors:  10% 94.4M/988M [00:02<00:20, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  11% 105M/988M [00:02<00:20, 42.7MB/s] \u001b[A\n",
            "model.safetensors:  12% 115M/988M [00:02<00:20, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  13% 126M/988M [00:02<00:20, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  14% 136M/988M [00:03<00:19, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  15% 147M/988M [00:03<00:19, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  16% 157M/988M [00:03<00:19, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  17% 168M/988M [00:03<00:19, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  18% 178M/988M [00:04<00:19, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  19% 189M/988M [00:04<00:18, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  20% 199M/988M [00:04<00:18, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  21% 210M/988M [00:04<00:18, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  22% 220M/988M [00:05<00:17, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  23% 231M/988M [00:05<00:17, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  24% 241M/988M [00:05<00:17, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  25% 252M/988M [00:05<00:17, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  27% 262M/988M [00:06<00:17, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  28% 273M/988M [00:06<00:16, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  29% 283M/988M [00:06<00:16, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  30% 294M/988M [00:06<00:16, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  31% 304M/988M [00:07<00:16, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  32% 315M/988M [00:07<00:15, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  33% 325M/988M [00:07<00:15, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  34% 336M/988M [00:07<00:15, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  35% 346M/988M [00:08<00:15, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  36% 357M/988M [00:08<00:14, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  37% 367M/988M [00:08<00:14, 42.9MB/s]\u001b[A\n",
            "model.safetensors:  38% 377M/988M [00:08<00:14, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  39% 388M/988M [00:09<00:14, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  40% 398M/988M [00:09<00:13, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  41% 409M/988M [00:09<00:13, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  42% 419M/988M [00:09<00:13, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  44% 430M/988M [00:10<00:13, 41.0MB/s]\u001b[A\n",
            "model.safetensors:  45% 440M/988M [00:10<00:13, 39.9MB/s]\u001b[A\n",
            "model.safetensors:  46% 451M/988M [00:10<00:13, 39.8MB/s]\u001b[A\n",
            "model.safetensors:  47% 461M/988M [00:11<00:14, 36.6MB/s]\u001b[A\n",
            "model.safetensors:  48% 472M/988M [00:11<00:14, 34.7MB/s]\u001b[A\n",
            "model.safetensors:  49% 482M/988M [00:11<00:14, 34.8MB/s]\u001b[A\n",
            "model.safetensors:  50% 493M/988M [00:11<00:13, 36.5MB/s]\u001b[A\n",
            "model.safetensors:  51% 503M/988M [00:12<00:12, 37.9MB/s]\u001b[A\n",
            "model.safetensors:  52% 514M/988M [00:12<00:12, 39.2MB/s]\u001b[A\n",
            "model.safetensors:  53% 524M/988M [00:12<00:11, 40.1MB/s]\u001b[A\n",
            "model.safetensors:  54% 535M/988M [00:12<00:11, 40.9MB/s]\u001b[A\n",
            "model.safetensors:  55% 545M/988M [00:13<00:10, 41.6MB/s]\u001b[A\n",
            "model.safetensors:  56% 556M/988M [00:13<00:10, 42.0MB/s]\u001b[A\n",
            "model.safetensors:  57% 566M/988M [00:13<00:09, 42.3MB/s]\u001b[A\n",
            "model.safetensors:  58% 577M/988M [00:13<00:09, 42.4MB/s]\u001b[A\n",
            "model.safetensors:  59% 587M/988M [00:14<00:09, 42.2MB/s]\u001b[A\n",
            "model.safetensors:  60% 598M/988M [00:14<00:09, 42.4MB/s]\u001b[A\n",
            "model.safetensors:  62% 608M/988M [00:14<00:08, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  63% 619M/988M [00:14<00:08, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  64% 629M/988M [00:15<00:08, 42.1MB/s]\u001b[A\n",
            "model.safetensors:  65% 640M/988M [00:15<00:08, 42.4MB/s]\u001b[A\n",
            "model.safetensors:  66% 650M/988M [00:15<00:07, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  67% 661M/988M [00:15<00:07, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  68% 671M/988M [00:16<00:07, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  69% 682M/988M [00:16<00:07, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  70% 692M/988M [00:16<00:06, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  71% 703M/988M [00:16<00:06, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  72% 713M/988M [00:17<00:06, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  73% 724M/988M [00:17<00:06, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  74% 734M/988M [00:17<00:05, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  75% 744M/988M [00:17<00:05, 42.3MB/s]\u001b[A\n",
            "model.safetensors:  76% 755M/988M [00:18<00:05, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  77% 765M/988M [00:18<00:05, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  79% 776M/988M [00:18<00:04, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  80% 786M/988M [00:18<00:04, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  81% 797M/988M [00:19<00:04, 39.4MB/s]\u001b[A\n",
            "model.safetensors:  82% 807M/988M [00:19<00:04, 43.8MB/s]\u001b[A\n",
            "model.safetensors:  83% 818M/988M [00:19<00:03, 43.3MB/s]\u001b[A\n",
            "model.safetensors:  84% 828M/988M [00:19<00:03, 43.2MB/s]\u001b[A\n",
            "model.safetensors:  85% 839M/988M [00:20<00:03, 42.9MB/s]\u001b[A\n",
            "model.safetensors:  86% 849M/988M [00:20<00:03, 42.2MB/s]\u001b[A\n",
            "model.safetensors:  87% 860M/988M [00:20<00:03, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  88% 870M/988M [00:20<00:02, 42.7MB/s]\u001b[A\n",
            "model.safetensors:  89% 881M/988M [00:21<00:02, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  90% 891M/988M [00:21<00:02, 42.5MB/s]\u001b[A\n",
            "model.safetensors:  91% 902M/988M [00:21<00:02, 42.6MB/s]\u001b[A\n",
            "model.safetensors:  92% 912M/988M [00:21<00:01, 41.4MB/s]\u001b[A\n",
            "model.safetensors:  93% 923M/988M [00:22<00:01, 40.4MB/s]\u001b[A\n",
            "model.safetensors:  94% 933M/988M [00:22<00:01, 38.0MB/s]\u001b[A\n",
            "model.safetensors:  96% 944M/988M [00:22<00:01, 33.9MB/s]\u001b[A\n",
            "model.safetensors:  97% 954M/988M [00:22<00:00, 36.2MB/s]\u001b[A\n",
            "model.safetensors:  98% 965M/988M [00:23<00:00, 37.9MB/s]\u001b[A\n",
            "model.safetensors:  99% 975M/988M [00:23<00:00, 39.2MB/s]\u001b[A\n",
            "model.safetensors: 100% 988M/988M [00:23<00:00, 41.5MB/s]\n",
            "Download complete. Moving file to /root/models/Qwen2.5-0.5B-Instruct/model.safetensors\n",
            "Fetching 10 files: 100% 10/10 [00:24<00:00,  2.40s/it]\n",
            "/root/models/Qwen2.5-0.5B-Instruct\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct --local-dir $HOME/models/Qwen2.5-0.5B-Instruct\n",
        "\n",
        "# If huggingface-cli is not stable, use the method below\n",
        "# import transformers\n",
        "# transformers.pipeline('text-generation', model='Qwen/Qwen2.5-0.5B-Instruct')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWlDQ6EjnBWz"
      },
      "source": [
        "# Dataset preparation\n",
        "\n",
        "We train with the Grade School Math 8K (GSM8k) task in this demo. The dataset is downloaded from huggingface [gsm8k](https://huggingface.co/datasets/openai/gsm8k) and below are some samples:\n",
        "\n",
        "\n",
        "**Prompt**\n",
        "\n",
        "Katy makes coffee using teaspoons of sugar and cups of water in the ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups of water, calculate the number of teaspoonfuls of sugar she used.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "The total ratio representing the ingredients she used to make the coffee is 7+13 = <<7+13=20>>20 Since the fraction representing the number of teaspoons she used is 7/20, she used 7/20120 = <<7/20120=42>>42 #### 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgRCvb6V6B3A",
        "outputId": "c5d78611-7bd3-48b0-f537-f567577a0b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md: 100% 7.94k/7.94k [00:00<00:00, 32.2MB/s]\n",
            "train-00000-of-00001.parquet: 100% 2.31M/2.31M [00:00<00:00, 38.9MB/s]\n",
            "test-00000-of-00001.parquet: 100% 419k/419k [00:00<00:00, 25.2MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 142352.53 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 287675.47 examples/s]\n",
            "Map: 100% 7473/7473 [00:00<00:00, 18604.25 examples/s]\n",
            "Map: 100% 1319/1319 [00:00<00:00, 19562.75 examples/s]\n",
            "Creating parquet from Arrow format: 100% 8/8 [00:00<00:00, 209.83ba/s]\n",
            "Creating parquet from Arrow format: 100% 2/2 [00:00<00:00, 295.71ba/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p $HOME/data/gsm8k\n",
        "!python3 $HOME/verl_repo/examples/data_preprocess/gsm8k.py --local_dir $HOME/data/gsm8k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPZZKBxunAoj"
      },
      "source": [
        "# the reward\n",
        "\n",
        "We use a rule-based reward model. We force the model to produce a final answer following 4 `#` as shown in the solution. We extract the final answer from both the solution and model's output using regular expression matching. We compare them and assign a reward of 1 to correct answer, 0.1 to incorrect answer and 0 to no answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjjLVuO60WD1",
        "outputId": "6e4c9b3a-8e5f-4c08-b44d-770f08a13c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def compute_score(solution_str, ground_truth, method='strict', format_score=0., score=1.):\n",
            "    \"\"\"The scoring function for GSM8k.\n",
            "\n",
            "    Reference: Trung, Luong, et al. \"Reft: Reasoning with reinforced fine-tuning.\" Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024.\n",
            "\n",
            "    Args:\n",
            "        solution_str: the solution text\n",
            "        ground_truth: the ground truth\n",
            "        method: the method to extract the solution, choices are 'strict' and 'flexible'\n",
            "        format_score: the score for the format\n",
            "        score: the score for the correct answer\n",
            "    \"\"\"\n",
            "    answer = extract_solution(solution_str=solution_str, method=method)\n",
            "    if answer is None:\n",
            "        return 0\n",
            "    else:\n",
            "        if answer == ground_truth:\n",
            "            return score\n",
            "        else:\n",
            "            return format_score\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "from verl.utils.reward_score.gsm8k import compute_score as gsm8k_reward\n",
        "print(inspect.getsource(gsm8k_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPBGPdSD0sCF"
      },
      "source": [
        "# Run the RL Pipeline\n",
        "Let's start with the Proximal Policy Optimization (PPO) algorithm,  one of the most widely used methods for post-training large language models.\n",
        "\n",
        "The main entry point of the PPO algorithm example is: `main_ppo.py`. A detailed guide to understanding the code architecture of `main_ppo.py` is available [here](https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html).\n",
        "\n",
        "In this tutorial, we will demonstrate how to run the PPO algorithm with **Qwen 2.5-0.5B** by setting:\n",
        "- `trainer.n_gpus_per_node`: Number of GPUs per node.\n",
        "\n",
        "- `actor_rollout_ref.rollout.tensor_model_parallel_size`: TP size for rollout. Only effective for vllm.\n",
        "\n",
        "- `actor_rollout_ref/critic.model.path`: Huggingface model path. This can be either local path or HDFS path. For HDFS path, we provide utils to download it to DRAM and convert the HDFS path to local path.\n",
        "\n",
        "- `data.train_batch_size`: Batch size sampled for one training iteration of different RL algorithms.\n",
        "\n",
        "- `data.max_prompt_length`: Maximum prompt length. All prompts will be left-padded to this length. An error will be reported if the length is too long.\n",
        "\n",
        "- `data.max_response_length`: Maximum response length. Rollout in RL algorithms (e.g. PPO) generates up to this length.\n",
        "\n",
        "- `actor_rollout_ref.actor.ppo_mini_batch_size`: One sample is split into multiple sub-batches with batch_size=ppo_mini_batch_size for PPO updates.\n",
        "\n",
        "- `actor_rollout_ref/critic.actor.ppo_micro_batch_size`: Similar to gradient accumulation, the micro_batch_size for one forward pass, trading speed for GPU memory.\n",
        "\n",
        "The full configuration explanation is available [here](https://verl.readthedocs.io/en/latest/examples/config.html).\n",
        "\n",
        "The training may take a few hours to finish but you can observe how the model performance increases. It will progressively output:\n",
        "\n",
        "- generated sentences.\n",
        "\n",
        "- step information with RL metrics, such as entropy loss, kl, and ``val/test_score/openai/gsm8k`` (validated every ``trainer.test_freq`` steps)\n",
        "\n",
        "If you come across GPU out of memory issues, set smaller values for the micro batch size used for gradient accumulation:\n",
        "\n",
        "- actor_rollout_ref.actor.ppo_micro_batch_size=1\n",
        "- critic.ppo_micro_batch_size=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvyEebBB4eCA",
        "outputId": "eb8cbcff-8648-4fa3-e71b-12aa8879f575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "2025-02-15 17:10:50.360354: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739639450.574127    3328 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739639450.630364    3328 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-15 17:10:51.087889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-02-15 17:10:58,434\tINFO worker.py:1841 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=3625)\u001b[0m 2025-02-15 17:11:07.162175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=3625)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=3625)\u001b[0m E0000 00:00:1739639467.182937    3625 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=3625)\u001b[0m E0000 00:00:1739639467.190947    3625 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'entropy_coeff': 0.001,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                                  'grad_offload': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                                  'optimizer_offload': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                                  'param_offload': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                                  'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'grad_clip': 1.0,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'kl_loss_coef': 0.001,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'kl_loss_type': 'low_var_kl',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'optim': {'lr': 1e-06,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                            'lr_warmup_steps_ratio': 0.0,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                            'min_lr_ratio': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                            'total_training_steps': -1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                            'warmup_style': 'constant'},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'ppo_epochs': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'ppo_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'ppo_micro_batch_size': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'ppo_micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'ppo_mini_batch_size': 64,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'shuffle': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'strategy': 'fsdp',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'use_dynamic_bsz': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'use_kl_loss': False},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                        'hybrid_engine': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                        'model': {'enable_gradient_checkpointing': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'external_lib': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'override_config': {},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'path': '/root/models/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                  'use_remove_padding': False},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                        'ref': {'fsdp_config': {'param_offload': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                                'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                'log_prob_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                'log_prob_micro_batch_size': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                'log_prob_micro_batch_size_per_gpu': 4,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                'log_prob_use_dynamic_bsz': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                'ulysses_sequence_parallel_size': 1},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                        'rollout': {'disable_log_stats': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'do_sample': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'dtype': 'bfloat16',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'enable_chunked_prefill': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'enforce_eager': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'free_cache_engine': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'gpu_memory_utilization': 0.4,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'ignore_eos': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'load_format': 'dummy_dtensor',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'log_prob_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'log_prob_micro_batch_size': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'log_prob_micro_batch_size_per_gpu': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'log_prob_use_dynamic_bsz': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'max_num_batched_tokens': 8192,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'max_num_seqs': 1024,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'n': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'name': 'vllm',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'prompt_length': 512,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'response_length': 256,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'temperature': 1.0,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'tensor_model_parallel_size': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'top_k': -1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                    'top_p': 1}},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m  'algorithm': {'adv_estimator': 'gae',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                'gamma': 1.0,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                'kl_penalty': 'kl',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                'lam': 1.0},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m  'critic': {'cliprange_value': 0.5,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'forward_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'forward_micro_batch_size': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'forward_micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'grad_clip': 1.0,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'model': {'enable_gradient_checkpointing': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'external_lib': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                       'grad_offload': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                       'optimizer_offload': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                       'param_offload': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                       'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'override_config': {},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'path': '/root/models/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'tokenizer_path': '/root/models/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'use_remove_padding': False},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'optim': {'lr': 1e-05,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'lr_warmup_steps_ratio': 0.0,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'min_lr_ratio': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'total_training_steps': -1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                       'warmup_style': 'constant'},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'ppo_epochs': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'ppo_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'ppo_micro_batch_size': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'ppo_micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'ppo_mini_batch_size': 64,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'shuffle': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'strategy': 'fsdp',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m             'use_dynamic_bsz': False},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m  'data': {'max_prompt_length': 512,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'max_response_length': 256,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'prompt_key': 'prompt',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'return_raw_chat': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'return_raw_input_ids': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'shuffle': True,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'tokenizer': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'train_batch_size': 256,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'train_files': '/root/data/gsm8k/train.parquet',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'val_batch_size': 1312,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m           'val_files': '/root/data/gsm8k/test.parquet'},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m  'reward_model': {'enable': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'forward_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'max_length': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'micro_batch_size': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'model': {'external_lib': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                             'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                             'min_num_params': 0,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                                             'param_offload': False},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                             'input_tokenizer': '/root/models/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                             'use_remove_padding': False},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'reward_manager': 'naive',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'strategy': 'fsdp',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m                   'use_dynamic_bsz': False},\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m  'trainer': {'critic_warmup': 0,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'default_hdfs_dir': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'default_local_dir': 'checkpoints/verl_examples/gsm8k',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'del_local_ckpt_after_load': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'experiment_name': 'gsm8k',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'logger': ['console'],\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'n_gpus_per_node': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'nnodes': 1,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'project_name': 'verl_examples',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'remove_previous_ckpt_in_save': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'resume_from_path': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'resume_mode': 'auto',\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'save_freq': 10,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'test_freq': 10,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'total_epochs': 15,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'total_training_steps': None,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'val_before_train': False,\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m              'val_generations_to_log_to_wandb': 0}}\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m /usr/local/lib/python3.11/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m No module named 'vllm._version'\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m   from vllm.version import __version__ as VLLM_VERSION\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m [validate_config] All configuration checks passed successfully!\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m original dataset len: 7473\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m filter dataset len: 7473\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m original dataset len: 1319\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m filter dataset len: 1319\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m Size of train dataloader: 29\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m Size of val dataloader: 1\n",
            "\u001b[36m(main_task pid=3625)\u001b[0m Total training steps: 435\n",
            "\u001b[36m(pid=3786)\u001b[0m 2025-02-15 17:11:23.790678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=3786)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=3786)\u001b[0m E0000 00:00:1739639483.811144    3786 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=3786)\u001b[0m E0000 00:00:1739639483.817327    3786 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=3786)\u001b[0m /usr/local/lib/python3.11/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
            "\u001b[36m(pid=3786)\u001b[0m No module named 'vllm._version'\n",
            "\u001b[36m(pid=3786)\u001b[0m   from vllm.version import __version__ as VLLM_VERSION\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Critic overriding config {'bos_token_id': None, 'eos_token_id': 151645, 'pad_token_id': 151643}\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Qwen2ForTokenClassification contains 494.03M parameters\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Before critic FSDP, memory allocated (GB): 0.0, memory reserved (GB): 0.0\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at /root/models/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m NCCL version 2.20.5+cuda12.4\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m After critic FSDP, memory allocated (GB): 1.8410954475402832, memory reserved (GB): 3.013671875\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Total steps: 435, num_warmup_steps: 0\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Critic use_remove_padding=False\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Model config after override: Qwen2Config {\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"_name_or_path\": \"/root/models/Qwen2.5-0.5B-Instruct\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"architectures\": [\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m     \"Qwen2ForCausalLM\"\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   ],\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"attention_dropout\": 0.0,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"eos_token_id\": 151645,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"hidden_act\": \"silu\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"hidden_size\": 896,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"initializer_range\": 0.02,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"intermediate_size\": 4864,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"max_position_embeddings\": 32768,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"max_window_layers\": 21,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"model_type\": \"qwen2\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"num_attention_heads\": 14,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"num_hidden_layers\": 24,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"num_key_value_heads\": 2,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"pad_token_id\": 151643,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"rope_scaling\": null,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"rope_theta\": 1000000.0,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"sliding_window\": null,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"tie_word_embeddings\": true,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"transformers_version\": \"4.47.1\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"use_cache\": true,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"use_sliding_window\": false,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"vocab_size\": 151936\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m }\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m \n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Qwen2ForCausalLM contains 494.03M parameters\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m wrap_policy: functools.partial(<function _or_policy at 0x7c0cd7773ba0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7c0cd7773a60>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Actor use_remove_padding=False\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Model config after override: Qwen2Config {\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"_name_or_path\": \"/root/models/Qwen2.5-0.5B-Instruct\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"architectures\": [\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m     \"Qwen2ForCausalLM\"\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   ],\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"attention_dropout\": 0.0,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"eos_token_id\": 151645,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"hidden_act\": \"silu\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"hidden_size\": 896,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"initializer_range\": 0.02,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"intermediate_size\": 4864,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"max_position_embeddings\": 32768,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"max_window_layers\": 21,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"model_type\": \"qwen2\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"num_attention_heads\": 14,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"num_hidden_layers\": 24,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"num_key_value_heads\": 2,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"pad_token_id\": 151643,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"rope_scaling\": null,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"rope_theta\": 1000000.0,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"sliding_window\": null,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"tie_word_embeddings\": true,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"transformers_version\": \"4.47.1\",\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"use_cache\": true,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"use_sliding_window\": false,\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   \"vocab_size\": 151936\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m }\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m \n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Qwen2ForCausalLM contains 494.03M parameters\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m wrap_policy: functools.partial(<function _or_policy at 0x7c0cd7773ba0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7c0cd7773a60>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Total steps: 435, num_warmup_steps: 0\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Actor use_remove_padding=False\n",
            "\u001b[36m(WorkerDict pid=3786)\u001b[0m Before building vllm rollout, memory allocated (GB): 4.602750778198242, memory reserved (GB): 5.78125\n"
          ]
        }
      ],
      "source": [
        "!PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n",
        " data.train_files=$HOME/data/gsm8k/train.parquet \\\n",
        " data.val_files=$HOME/data/gsm8k/test.parquet \\\n",
        " data.train_batch_size=256 \\\n",
        " data.val_batch_size=1312 \\\n",
        " data.max_prompt_length=512 \\\n",
        " data.max_response_length=256 \\\n",
        " actor_rollout_ref.model.path=$HOME/models/Qwen2.5-0.5B-Instruct \\\n",
        " actor_rollout_ref.actor.optim.lr=1e-6 \\\n",
        " actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n",
        " actor_rollout_ref.actor.ppo_micro_batch_size=1 \\\n",
        " actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=1 \\\n",
        " actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n",
        " actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n",
        " actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n",
        " critic.optim.lr=1e-5 \\\n",
        " critic.model.path=$HOME/models/Qwen2.5-0.5B-Instruct \\\n",
        " critic.ppo_micro_batch_size=1 \\\n",
        " algorithm.kl_ctrl.kl_coef=0.001 \\\n",
        " +trainer.val_before_train=False \\\n",
        " trainer.default_hdfs_dir=null \\\n",
        " trainer.n_gpus_per_node=1 \\\n",
        " trainer.nnodes=1 \\\n",
        " trainer.save_freq=10 \\\n",
        " trainer.test_freq=10 \\\n",
        " trainer.total_epochs=15 \\\n",
        " trainer.logger=\\[console\\]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSn7lNlZ2vfL"
      },
      "source": [
        "# Stop and clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuJ-LgdTAPkb"
      },
      "outputs": [],
      "source": [
        "!ray stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOVMN5OJZEo4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}